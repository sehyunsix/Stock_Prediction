{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "from torchsummary import summary\n",
    "from TCN import TemporalConvNet\n",
    "import argparse\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset ,DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "DEVICE = 'cuda:0'\n",
    "IR = 0.1\n",
    "EPOCH =400\n",
    "DROPOUT=0.3\n",
    "PATIENCE =20\n",
    "BATCH_SIZE = 1024\n",
    "NUM_LAYERS =2\n",
    "SUFFLE = True\n",
    "WINDOW_SIZE  = 90\n",
    "PREDICT_SIZE = 15\n",
    "SLIDING_SIZE =1\n",
    "FEATURE_SIZE = 5\n",
    "BATCH_SIZE =64\n",
    "FULLLY_SIZE =128\n",
    "FEATURE = [ 'Open', 'High','Low','Close', 'Volume']\n",
    "TARGET = 'Close'\n",
    "TICKER_NUMBER =2136\n",
    "TOTAL_DAY = 755\n",
    "WINDOW_NUMBER =int(TOTAL_DAY - (WINDOW_SIZE+PREDICT_SIZE)/SLIDING_SIZE+1)\n",
    "\n",
    "\n",
    "\n",
    "#model\n",
    "input_size = 5\n",
    "num_channels =[80]*4\n",
    "kenel_size = 7\n",
    "output_size= 30\n",
    "dropout = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCNModel, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.linear.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.tcn(x)\n",
    "        return self.linear(y1[:, :, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(650, 2136, 105, 5) (1, 2136, 105, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data= pd.read_csv('../../../data/NASDAQ_3y/stock_price_clean.csv',index_col=0)\n",
    "# data.columns = ['Date', 'Ticker', 'Open', 'High','Low', 'Close', 'Volume','sell','buy' ]\n",
    "\n",
    "\n",
    "def min_max(sequences):\n",
    "  results = sequences.copy()\n",
    "  v_min =results.min()\n",
    "  v_max =results.max()\n",
    "  new_min =0\n",
    "  new_max =1\n",
    "  min_max=[]\n",
    "  for index,sequence in enumerate(results):\n",
    "    v_min =sequence.min()\n",
    "    v_max =sequence.max()\n",
    "    v_p = (sequence - v_min)/(v_max - v_min)*(new_max - new_min) + new_min\n",
    "    min_max.append([v_min,v_max])\n",
    "    results[index] = v_p\n",
    "  return results, min_max\n",
    "\n",
    "def inverse_min_max(sequences,min_max):\n",
    "  original = []\n",
    "  for index,sequence in enumerate(sequences):\n",
    "    v_min =min_max[index][0]\n",
    "    v_max =min_max[index][1]\n",
    "    sequence_restored = (sequence) * (v_max - v_min) + v_min\n",
    "    original.append(sequence_restored)\n",
    "  return np.array(original)\n",
    "\n",
    "def preprocess_lstm(df):\n",
    "  sequences = list()\n",
    "  for group in df.groupby('Ticker'):\n",
    "    sequences.append(group[1][FEATURE])\n",
    "  sequences=np.array(sequences)\n",
    "\n",
    "  ## min_max trading\n",
    "  price , trade,sentiments =np.split(sequences,[4,5],axis=2)\n",
    "  trade , min_max_trading =min_max(trade)\n",
    "  price , min_max_list =min_max(price)\n",
    "  combine =np.concatenate([price,trade,sentiments],axis =2)\n",
    "\n",
    "  ## min_max another\n",
    "\n",
    "  ## min_max another\n",
    "  result_list= []\n",
    "  for i in range(0,WINDOW_NUMBER):\n",
    "    a, b, c= np.split(combine,[i,i+WINDOW_SIZE+PREDICT_SIZE],axis=1)\n",
    "    result_list.append(b)\n",
    "  result_array = np.array(result_list)\n",
    "  print(166//(WINDOW_SIZE+PREDICT_SIZE))\n",
    "  # train ,vaild = np.split(result_array,[TOTAL_DAY%(WINDOW_SIZE+PREDICT_SIZE)],axis=0)\n",
    "  train ,vaild = np.split(result_array,[-1],axis=0)\n",
    "\n",
    "  print(train.shape ,vaild.shape)\n",
    "  # train_x,train_y=np.split(train.reshape(TOTAL_DAY%(WINDOW_SIZE+PREDICT_SIZE)*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "  train_x,train_y=np.split(train.reshape((WINDOW_NUMBER-1)*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "  vaild_x,vaild_y=np.split(vaild.reshape(1*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "  return train_x,train_y,vaild_x,vaild_y,min_max_list\n",
    "\n",
    "\n",
    "\n",
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, target):\n",
    "        self.x_data = torch.tensor(x_data,dtype =torch.float32)\n",
    "        self.y_data = torch.tensor(y_data[:,FEATURE.index(target),:],dtype = torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x_data.shape[0]\n",
    "train_x,train_y,vaild_x,vaild_y,min_max_list = preprocess_lstm(data)\n",
    "\n",
    "def swap_axes(arr):\n",
    "    return np.swapaxes(arr, 1, 2)\n",
    "train_x = swap_axes(train_x)\n",
    "train_y = swap_axes(train_y)\n",
    "vaild_x = swap_axes(vaild_x)\n",
    "vaild_y = swap_axes(vaild_y)\n",
    "train_data = BaseDataset(train_x,train_y,TARGET)\n",
    "test_data =  BaseDataset(vaild_x,vaild_y,TARGET)\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1388400, 5, 90), (1388400, 5, 15), (2136, 5, 90), (2136, 5, 15), 2136)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape,train_y.shape,vaild_x.shape,vaild_y.shape,len(min_max_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCN_trainer:\n",
    "    def __init__(self,model, train_loader, test_loader,num_epochs = None, lr = None,batch_size=BATCH_SIZE, verbose = 1, patience=None):\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader =test_loader\n",
    "        self.verbose = verbose\n",
    "        self.patience =patience\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.criterion = nn.MSELoss().to(self.device)\n",
    "        self.optimizer =  optim.Adam(model.parameters(), lr = lr)\n",
    "        self.scheduler =   optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min')\n",
    "        self.nb_epochs = num_epochs\n",
    "        self.train_hist  = np.zeros(self.nb_epochs)\n",
    "        self.vaild_hist =[]\n",
    "\n",
    "    def train(self):\n",
    "        config = {\n",
    "        \"model\":'TCN',\n",
    "        \"learning_rate\": IR,\n",
    "        \"epochs\": EPOCH,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"shuffle\":SUFFLE,\n",
    "        'verbose':1,\n",
    "        'patience':PATIENCE,\n",
    "        'dropout':DROPOUT,\n",
    "        'target':TARGET,\n",
    "        'feature_columns':FEATURE,\n",
    "        \"NUM_CHANNELS\":num_channels,\n",
    "        'KERNEL':kenel_size,\n",
    "\n",
    "        }\n",
    "        run = wandb.init(project =\"LSTM\",config=config)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        for epoch in range(self.nb_epochs):\n",
    "            avg_cost = 0\n",
    "            total_batch = len(self.train_loader)\n",
    "            for batch_idx, samples in enumerate(self.train_loader):\n",
    "                x_train, y_train = samples\n",
    "                x_train =x_train.cuda()\n",
    "                y_train =y_train.cuda()\n",
    "                outputs = self.model(x_train)\n",
    "                loss = self.criterion(outputs, y_train)\n",
    "                self.optimizer.zero_grad()\n",
    "                ##loss\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                avg_cost += loss/total_batch\n",
    "                ##정확도 계산\n",
    "                predict, label = outputs.clone().detach(),y_train.clone().detach()\n",
    "                predict = inverse_min_max(predict.to('cpu').numpy(),min_max_list)\n",
    "                label =  inverse_min_max(label.to('cpu').numpy(),min_max_list)\n",
    "                score = self.MAE(predict,label)\n",
    "                acc = self.accuracy(label,predict)\n",
    "                wandb.log({\"Training Loss\": loss.item()})\n",
    "            self.train_hist[epoch] = avg_cost\n",
    "            if epoch % self.verbose == 0:\n",
    "                total_loss, score,acc =self.vaild()\n",
    "                print('Epoch:', '%04d' % (epoch), 'train loss :', '{:.4f}'.format(avg_cost))\n",
    "                print('vaild_loss:', '{:.4f}'.format (total_loss), 'MAE :', '{:.4f}'.format(score),'Accuarcy :', '{:.4f}'.format(acc))\n",
    "                wandb.log({\"Evaluation Loss\": loss.item()})\n",
    "                wandb.log({\"Evaluation MAE\": score.item()})\n",
    "                wandb.log({\"Evaluation Accuracy\": acc.item()})\n",
    "                self.vaild_hist.append({\"vaild_loss\":total_loss ,\"vaild_score\":score,\"vaild_accuracy\":acc,\"epoch\":epoch})\n",
    "                self.scheduler.step(total_loss)\n",
    "        # patience번째 마다 early stopping 여부 확인\n",
    "            if (epoch % self.patience == 0) & (epoch != 0):\n",
    "                # loss가 커졌다면 early stop\n",
    "                index =int(epoch/self.patience)\n",
    "                if index> 1:\n",
    "                    print((self.vaild_hist[index-1]['vaild_loss'] ,  self.vaild_hist[index]['vaild_loss'])   )\n",
    "                    if self.vaild_hist[index-1]['vaild_loss']<self.vaild_hist[index]['vaild_loss']:\n",
    "                        print('\\n Early Stopping')\n",
    "                        break\n",
    "                    else:\n",
    "                        print('model was saved')\n",
    "                        torch.save(self.model,\"best-model.pt\")\n",
    "    def vaild(self):\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = []\n",
    "            avg_cost = 0\n",
    "            score = 0\n",
    "            acc = 0\n",
    "            total_batch= len(test_dataloader)\n",
    "            for batch_idx, samples in enumerate(self.test_loader):\n",
    "                x_test, y_test = samples\n",
    "                x_test =x_test.cuda()\n",
    "                y_test =y_test.cuda()\n",
    "                outputs = self.model(x_test)\n",
    "                loss = self.criterion(outputs, y_test)\n",
    "                avg_cost += loss\n",
    "                predict = inverse_min_max(outputs.to('cpu'),min_max_list)\n",
    "                y_test =  inverse_min_max(y_test.to('cpu'),min_max_list)\n",
    "                score += self.MAE(predict,y_test)\n",
    "                acc += self.accuracy(y_test,predict)\n",
    "        total_loss = avg_cost/total_batch\n",
    "        score = score/len(self.test_loader)\n",
    "        acc = acc/len(self.test_loader)\n",
    "        self.model.train()\n",
    "        return total_loss,score,acc\n",
    "\n",
    "    def MAE(self,true, pred):\n",
    "         return np.mean(np.abs(true-pred))\n",
    "    def accuracy(self,true, pred):\n",
    "        return (1-np.mean(np.abs((true-pred)/true)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssu36/anaconda3/envs/tiger/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/home/ssu36/anaconda3/envs/tiger/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model = TCNModel(input_size=5 ,num_channels=num_channels,output_size=PREDICT_SIZE,kernel_size=kenel_size, dropout=0.25)\n",
    "trainer = TCN_trainer(model,train_dataloader,test_dataloader,num_epochs=EPOCH,lr= IR,batch_size=BATCH_SIZE,patience=PATIENCE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msdfg8931\u001b[0m (\u001b[33msehyunai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ssu36/tiger/Stock_Prediction/models/TimeSeries/TCN/wandb/run-20231124_211953-r8d6pzm1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sehyunai/LSTM/runs/r8d6pzm1' target=\"_blank\">rich-cosmos-110</a></strong> to <a href='https://wandb.ai/sehyunai/LSTM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sehyunai/LSTM' target=\"_blank\">https://wandb.ai/sehyunai/LSTM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sehyunai/LSTM/runs/r8d6pzm1' target=\"_blank\">https://wandb.ai/sehyunai/LSTM/runs/r8d6pzm1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 train loss : 1912651.5000\n",
      "vaild_loss: 0.0081 MAE : 19.8371 Accuarcy : 0.7875\n",
      "Epoch: 0001 train loss : 0.0045\n",
      "vaild_loss: 0.0048 MAE : 11.0538 Accuarcy : 0.7425\n",
      "Epoch: 0002 train loss : 0.0047\n",
      "vaild_loss: 0.0049 MAE : 12.2545 Accuarcy : 0.7621\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
