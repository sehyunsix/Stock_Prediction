{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssu36/anaconda3/envs/tiger/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "from torchsummary import summary\n",
    "from TCN import TemporalConvNet\n",
    "import argparse\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset ,DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "DEVICE = 'cuda:0'\n",
    "IR = 0.1\n",
    "EPOCH =400\n",
    "DROPOUT=0.3\n",
    "PATIENCE =20\n",
    "BATCH_SIZE = 1024\n",
    "NUM_LAYERS =2\n",
    "SUFFLE = True\n",
    "WINDOW_SIZE  = 90\n",
    "PREDICT_SIZE = 15\n",
    "SLIDING_SIZE =1\n",
    "FEATURE_SIZE = 5\n",
    "BATCH_SIZE =64\n",
    "FULLLY_SIZE =128\n",
    "FEATURE = [ 'Open', 'High','Low','Close', 'Volume']\n",
    "TARGET = 'Close'\n",
    "TICKER_NUMBER =2136\n",
    "TOTAL_DAY = 755\n",
    "WINDOW_NUMBER =int(TOTAL_DAY - (WINDOW_SIZE+PREDICT_SIZE)/SLIDING_SIZE+1)\n",
    "\n",
    "\n",
    "\n",
    "#model\n",
    "input_size = 5\n",
    "num_channels =[80]*4\n",
    "kenel_size = 7\n",
    "output_size= 30\n",
    "dropout = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCNModel, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.linear.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.tcn(x)\n",
    "        return self.linear(y1[:, :, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'polars'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/ssu36/tiger/Stock_Prediction/models/TimeSeries/TCN/TCN_test.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/models/TimeSeries/TCN/TCN_test.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpolars\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpl\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'polars'"
     ]
    }
   ],
   "source": [
    "\n",
    "import polars as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2//650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting polars\n",
      "  Obtaining dependency information for polars from https://files.pythonhosted.org/packages/0f/3c/6a9783afff586e5ea1c6ea12f462c1d839c1319447fe1e4d9a13e15c885c/polars-0.19.15-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading polars-0.19.15-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Downloading polars-0.19.15-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.4/28.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: polars\n",
      "Successfully installed polars-0.19.15\n"
     ]
    }
   ],
   "source": [
    "!pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = datasets.load_dataset('sehyun66/STOCKPRICE','NASDAQ_3y')\n",
    "import polars as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =data['train'].to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-23</td>\n",
       "      <td>538.859985</td>\n",
       "      <td>538.859985</td>\n",
       "      <td>516.799988</td>\n",
       "      <td>523.609985</td>\n",
       "      <td>523.609985</td>\n",
       "      <td>1764400</td>\n",
       "      <td>REGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-24</td>\n",
       "      <td>521.609985</td>\n",
       "      <td>528.440002</td>\n",
       "      <td>505.799988</td>\n",
       "      <td>506.100006</td>\n",
       "      <td>506.100006</td>\n",
       "      <td>1270300</td>\n",
       "      <td>REGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-25</td>\n",
       "      <td>507.179993</td>\n",
       "      <td>515.469971</td>\n",
       "      <td>507.179993</td>\n",
       "      <td>508.309998</td>\n",
       "      <td>508.309998</td>\n",
       "      <td>711800</td>\n",
       "      <td>REGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-27</td>\n",
       "      <td>513.419983</td>\n",
       "      <td>518.919983</td>\n",
       "      <td>509.200012</td>\n",
       "      <td>514.049988</td>\n",
       "      <td>514.049988</td>\n",
       "      <td>498200</td>\n",
       "      <td>REGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-30</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>520.780029</td>\n",
       "      <td>507.600006</td>\n",
       "      <td>516.030029</td>\n",
       "      <td>516.030029</td>\n",
       "      <td>1707500</td>\n",
       "      <td>REGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612675</th>\n",
       "      <td>2023-11-16</td>\n",
       "      <td>190.229996</td>\n",
       "      <td>193.830002</td>\n",
       "      <td>187.020004</td>\n",
       "      <td>189.169998</td>\n",
       "      <td>189.169998</td>\n",
       "      <td>184000</td>\n",
       "      <td>BGNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612676</th>\n",
       "      <td>2023-11-17</td>\n",
       "      <td>189.169998</td>\n",
       "      <td>189.884995</td>\n",
       "      <td>184.880005</td>\n",
       "      <td>186.880005</td>\n",
       "      <td>186.880005</td>\n",
       "      <td>306000</td>\n",
       "      <td>BGNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612677</th>\n",
       "      <td>2023-11-20</td>\n",
       "      <td>186.889999</td>\n",
       "      <td>188.649994</td>\n",
       "      <td>184.419998</td>\n",
       "      <td>187.880005</td>\n",
       "      <td>187.880005</td>\n",
       "      <td>418100</td>\n",
       "      <td>BGNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612678</th>\n",
       "      <td>2023-11-21</td>\n",
       "      <td>185.300003</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>183.675003</td>\n",
       "      <td>184.929993</td>\n",
       "      <td>184.929993</td>\n",
       "      <td>128800</td>\n",
       "      <td>BGNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612679</th>\n",
       "      <td>2023-11-22</td>\n",
       "      <td>184.729996</td>\n",
       "      <td>185.229996</td>\n",
       "      <td>181.550003</td>\n",
       "      <td>182.789993</td>\n",
       "      <td>182.789993</td>\n",
       "      <td>109600</td>\n",
       "      <td>BGNE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1612680 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Date        Open        High         Low       Close  \\\n",
       "0        2020-11-23  538.859985  538.859985  516.799988  523.609985   \n",
       "1        2020-11-24  521.609985  528.440002  505.799988  506.100006   \n",
       "2        2020-11-25  507.179993  515.469971  507.179993  508.309998   \n",
       "3        2020-11-27  513.419983  518.919983  509.200012  514.049988   \n",
       "4        2020-11-30  520.000000  520.780029  507.600006  516.030029   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "1612675  2023-11-16  190.229996  193.830002  187.020004  189.169998   \n",
       "1612676  2023-11-17  189.169998  189.884995  184.880005  186.880005   \n",
       "1612677  2023-11-20  186.889999  188.649994  184.419998  187.880005   \n",
       "1612678  2023-11-21  185.300003  187.000000  183.675003  184.929993   \n",
       "1612679  2023-11-22  184.729996  185.229996  181.550003  182.789993   \n",
       "\n",
       "          Adj Close   Volume Ticker  \n",
       "0        523.609985  1764400   REGN  \n",
       "1        506.100006  1270300   REGN  \n",
       "2        508.309998   711800   REGN  \n",
       "3        514.049988   498200   REGN  \n",
       "4        516.030029  1707500   REGN  \n",
       "...             ...      ...    ...  \n",
       "1612675  189.169998   184000   BGNE  \n",
       "1612676  186.880005   306000   BGNE  \n",
       "1612677  187.880005   418100   BGNE  \n",
       "1612678  184.929993   128800   BGNE  \n",
       "1612679  182.789993   109600   BGNE  \n",
       "\n",
       "[1612680 rows x 8 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ssu36/tiger/Stock_Prediction/models/TimeSeries/TCN/TCN_test.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/models/TimeSeries/TCN/TCN_test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m TOTAL_DAY \u001b[39m=\u001b[39m \u001b[39m755\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/models/TimeSeries/TCN/TCN_test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m WINDOW_NUMBER \u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(TOTAL_DAY \u001b[39m-\u001b[39m (WINDOW_SIZE\u001b[39m+\u001b[39mPREDICT_SIZE)\u001b[39m/\u001b[39mSLIDING_SIZE\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/models/TimeSeries/TCN/TCN_test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m data\u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39mload_dataset(\u001b[39m'\u001b[39m\u001b[39msehyun66/STOCKPRICE\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mNASDAQ_3y\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/models/TimeSeries/TCN/TCN_test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m data \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto_pandas()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/models/TimeSeries/TCN/TCN_test.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmin_max\u001b[39m(sequences):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "SUFFLE = False\n",
    "WINDOW_SIZE  = 90\n",
    "PREDICT_SIZE = 15\n",
    "SLIDING_SIZE =1\n",
    "FEATURE_SIZE = 5\n",
    "BATCH_SIZE =64\n",
    "FULLLY_SIZE =128\n",
    "FEATURE = [ 'Open', 'High','Low','Close', 'Volume']\n",
    "TARGET = 'Close'\n",
    "TICKER_NUMBER =2136\n",
    "TOTAL_DAY = 755\n",
    "WINDOW_NUMBER =int(TOTAL_DAY - (WINDOW_SIZE+PREDICT_SIZE)/SLIDING_SIZE+1)\n",
    "\n",
    "\n",
    "\n",
    "data= datasets.load_dataset('sehyun66/STOCKPRICE','NASDAQ_3y')\n",
    "data = data['train'].to_pandas()\n",
    "\n",
    "\n",
    "def min_max(sequences):\n",
    "  results = sequences.copy()\n",
    "  v_min =results.min()\n",
    "  v_max =results.max()\n",
    "  new_min =0\n",
    "  new_max =1\n",
    "  min_max=[]\n",
    "  for index,sequence in enumerate(results):\n",
    "    v_min =sequence.min()\n",
    "    v_max =sequence.max()\n",
    "    v_p = (sequence - v_min)/(v_max - v_min)*(new_max - new_min) + new_min\n",
    "    min_max.append([v_min,v_max])\n",
    "    results[index] = v_p\n",
    "  return results, min_max\n",
    "\n",
    "def inverse_min_max(sequences,min_max):\n",
    "  original = []\n",
    "  for index,sequence in enumerate(sequences):\n",
    "    v_min =min_max[index][0]\n",
    "    v_max =min_max[index][1]\n",
    "    sequence_restored = (sequence) * (v_max - v_min) + v_min\n",
    "    original.append(sequence_restored)\n",
    "  return np.array(original)\n",
    "\n",
    "def preprocess_lstm(df):\n",
    "  sequences = list()\n",
    "  for group in df.groupby('Ticker'):\n",
    "    sequences.append(group[1][FEATURE])\n",
    "  sequences=np.array(sequences)\n",
    "\n",
    "  ## min_max trading\n",
    "  price , trade,sentiments =np.split(sequences,[4,5],axis=2)\n",
    "  trade , min_max_trading =min_max(trade)\n",
    "  price , min_max_list =min_max(price)\n",
    "  combine =np.concatenate([price,trade,sentiments],axis =2)\n",
    "\n",
    "  ## min_max another\n",
    "  result_list= []\n",
    "  for i in range(0,WINDOW_NUMBER):\n",
    "    a, b, c= np.split(combine,[i,i+WINDOW_SIZE+PREDICT_SIZE],axis=1)\n",
    "    result_list.append(b)\n",
    "  result_array = np.array(result_list)\n",
    "  print(166//(WINDOW_SIZE+PREDICT_SIZE))\n",
    "  # train ,vaild = np.split(result_array,[TOTAL_DAY%(WINDOW_SIZE+PREDICT_SIZE)],axis=0)\n",
    "  train ,vaild = np.split(result_array,[-1],axis=0)\n",
    "\n",
    "  print(train.shape ,vaild.shape)\n",
    "  # train_x,train_y=np.split(train.reshape(TOTAL_DAY%(WINDOW_SIZE+PREDICT_SIZE)*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "  train_x,train_y=np.split(train.reshape((WINDOW_NUMBER-1)*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "  vaild_x,vaild_y=np.split(vaild.reshape(1*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "  return train_x,train_y,vaild_x,vaild_y,min_max_list\n",
    "\n",
    "\n",
    "\n",
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, target):\n",
    "        self.x_data = torch.tensor(x_data,dtype =torch.float32)\n",
    "        self.y_data = torch.tensor(y_data[:,FEATURE.index(target),:],dtype = torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x_data.shape[0]\n",
    "train_x,train_y,vaild_x,vaild_y,min_max_list = preprocess_lstm(data)\n",
    "\n",
    "def swap_axes(arr):\n",
    "    return np.swapaxes(arr, 1, 2)\n",
    "train_x = swap_axes(train_x)\n",
    "train_y = swap_axes(train_y)\n",
    "vaild_x = swap_axes(vaild_x)\n",
    "vaild_y = swap_axes(vaild_y)\n",
    "train_data = BaseDataset(train_x,train_y,TARGET)\n",
    "test_data =  BaseDataset(vaild_x,vaild_y,TARGET)\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False,drop_last=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False,drop_last=True)\n",
    "train_x.shape,train_y.shape,vaild_x.shape,vaild_y.shape,len(min_max_list)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssu36/anaconda3/envs/tiger/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755\n",
      "651\n",
      "(650, 2136, 105, 5) (1, 2136, 105, 5)\n",
      "651\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import sys\n",
    "sys.path.append(\"../../../\")\n",
    "from data.data_processing import preprocess_window\n",
    "\n",
    "\n",
    "\n",
    "SUFFLE = True\n",
    "WINDOW_SIZE  = 90\n",
    "PREDICT_SIZE = 15\n",
    "SLIDING_SIZE =1\n",
    "FEATURE_SIZE = 5\n",
    "BATCH_SIZE =64\n",
    "FULLLY_SIZE =128\n",
    "FEATURE = [ 'Open', 'High','Low','Close', 'Volume']\n",
    "TARGET = 'Close'\n",
    "TICKER_NUMBER =2136\n",
    "TOTAL_DAY = 755\n",
    "WINDOW_NUMBER =int(TOTAL_DAY - (WINDOW_SIZE+PREDICT_SIZE)/SLIDING_SIZE+1)\n",
    "data= datasets.load_dataset('sehyun66/STOCKPRICE','NASDAQ_3y')\n",
    "data = data['train'].to_pandas()\n",
    "# def preprocess_window(df,feature:list,TARGET:str,window_size:int,predict_size:int,sliding_size:int):\n",
    "\n",
    "train_x,train_y,vaild_x,vaild_y,min_max_list =preprocess_window(data,FEATURE,TARGET,WINDOW_SIZE,PREDICT_SIZE,SLIDING_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCN_trainer:\n",
    "    def __init__(self,model, train_loader, test_loader,num_epochs = None, lr = None,batch_size=BATCH_SIZE, verbose = 1, patience=None):\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader =test_loader\n",
    "        self.verbose = verbose\n",
    "        self.patience =patience\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.criterion = nn.MSELoss().to(self.device)\n",
    "        self.optimizer =  optim.Adam(model.parameters(), lr = lr)\n",
    "        self.scheduler =   optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min')\n",
    "        self.nb_epochs = num_epochs\n",
    "        self.train_hist  = np.zeros(self.nb_epochs)\n",
    "        self.vaild_hist =[]\n",
    "\n",
    "    def train(self):\n",
    "        config = {\n",
    "        \"model\":'TCN',\n",
    "        \"learning_rate\": IR,\n",
    "        \"epochs\": EPOCH,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"shuffle\":SUFFLE,\n",
    "        'verbose':1,\n",
    "        'patience':PATIENCE,\n",
    "        'dropout':DROPOUT,\n",
    "        'target':TARGET,\n",
    "        'feature_columns':FEATURE,\n",
    "        \"NUM_CHANNELS\":num_channels,\n",
    "        'KERNEL':kenel_size,\n",
    "\n",
    "        }\n",
    "        run = wandb.init(project =\"LSTM\",config=config)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        for epoch in range(self.nb_epochs):\n",
    "            avg_cost = 0\n",
    "            total_batch = len(self.train_loader)\n",
    "            for batch_idx, samples in enumerate(self.train_loader):\n",
    "                x_train, y_train = samples\n",
    "                x_train =x_train.cuda()\n",
    "                y_train =y_train.cuda()\n",
    "                outputs = self.model(x_train)\n",
    "                loss = self.criterion(outputs, y_train)\n",
    "                self.optimizer.zero_grad()\n",
    "                ##loss\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                avg_cost += loss/total_batch\n",
    "                ##정확도 계산\n",
    "                predict, label = outputs.clone().detach(),y_train.clone().detach()\n",
    "                predict = inverse_min_max(predict.to('cpu').numpy(),min_max_list)\n",
    "                label =  inverse_min_max(label.to('cpu').numpy(),min_max_list)\n",
    "                score = self.MAE(predict,label)\n",
    "                acc = self.accuracy(label,predict)\n",
    "                wandb.log({\"Training Loss\": loss.item()})\n",
    "            self.train_hist[epoch] = avg_cost\n",
    "            if epoch % self.verbose == 0:\n",
    "                total_loss, score,acc =self.vaild()\n",
    "                print('Epoch:', '%04d' % (epoch), 'train loss :', '{:.4f}'.format(avg_cost))\n",
    "                print('vaild_loss:', '{:.4f}'.format (total_loss), 'MAE :', '{:.4f}'.format(score),'Accuarcy :', '{:.4f}'.format(acc))\n",
    "                wandb.log({\"Evaluation Loss\": loss.item()})\n",
    "                wandb.log({\"Evaluation MAE\": score.item()})\n",
    "                wandb.log({\"Evaluation Accuracy\": acc.item()})\n",
    "                self.vaild_hist.append({\"vaild_loss\":total_loss ,\"vaild_score\":score,\"vaild_accuracy\":acc,\"epoch\":epoch})\n",
    "                self.scheduler.step(total_loss)\n",
    "        # patience번째 마다 early stopping 여부 확인\n",
    "            if (epoch % self.patience == 0) & (epoch != 0):\n",
    "                # loss가 커졌다면 early stop\n",
    "                index =int(epoch/self.patience)\n",
    "                if index> 1:\n",
    "                    print((self.vaild_hist[index-1]['vaild_loss'] ,  self.vaild_hist[index]['vaild_loss'])   )\n",
    "                    if self.vaild_hist[index-1]['vaild_loss']<self.vaild_hist[index]['vaild_loss']:\n",
    "                        print('\\n Early Stopping')\n",
    "                        break\n",
    "                    else:\n",
    "                        print('model was saved')\n",
    "                        torch.save(self.model,\"best-model.pt\")\n",
    "    def vaild(self):\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = []\n",
    "            avg_cost = 0\n",
    "            score = 0\n",
    "            acc = 0\n",
    "            total_batch= len(test_dataloader)\n",
    "            for batch_idx, samples in enumerate(self.test_loader):\n",
    "                x_test, y_test = samples\n",
    "                x_test =x_test.cuda()\n",
    "                y_test =y_test.cuda()\n",
    "                outputs = self.model(x_test)\n",
    "                loss = self.criterion(outputs, y_test)\n",
    "                avg_cost += loss\n",
    "                predict = inverse_min_max(outputs.to('cpu'),min_max_list)\n",
    "                y_test =  inverse_min_max(y_test.to('cpu'),min_max_list)\n",
    "                score += self.MAE(predict,y_test)\n",
    "                acc += self.accuracy(y_test,predict)\n",
    "        total_loss = avg_cost/total_batch\n",
    "        score = score/len(self.test_loader)\n",
    "        acc = acc/len(self.test_loader)\n",
    "        self.model.train()\n",
    "        return total_loss,score,acc\n",
    "\n",
    "    def MAE(self,true, pred):\n",
    "         return np.mean(np.abs(true-pred))\n",
    "    def accuracy(self,true, pred):\n",
    "        return (1-np.mean(np.abs((true-pred)/true)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssu36/anaconda3/envs/tiger/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/home/ssu36/anaconda3/envs/tiger/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model = TCNModel(input_size=5 ,num_channels=num_channels,output_size=PREDICT_SIZE,kernel_size=kenel_size, dropout=0.25)\n",
    "trainer = TCN_trainer(model,train_dataloader,test_dataloader,num_epochs=EPOCH,lr= IR,batch_size=BATCH_SIZE,patience=PATIENCE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msdfg8931\u001b[0m (\u001b[33msehyunai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ssu36/tiger/Stock_Prediction/models/TimeSeries/TCN/wandb/run-20231124_211953-r8d6pzm1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sehyunai/LSTM/runs/r8d6pzm1' target=\"_blank\">rich-cosmos-110</a></strong> to <a href='https://wandb.ai/sehyunai/LSTM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sehyunai/LSTM' target=\"_blank\">https://wandb.ai/sehyunai/LSTM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sehyunai/LSTM/runs/r8d6pzm1' target=\"_blank\">https://wandb.ai/sehyunai/LSTM/runs/r8d6pzm1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000 train loss : 1912651.5000\n",
      "vaild_loss: 0.0081 MAE : 19.8371 Accuarcy : 0.7875\n",
      "Epoch: 0001 train loss : 0.0045\n",
      "vaild_loss: 0.0048 MAE : 11.0538 Accuarcy : 0.7425\n",
      "Epoch: 0002 train loss : 0.0047\n",
      "vaild_loss: 0.0049 MAE : 12.2545 Accuarcy : 0.7621\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "class TCNargumenets():\n",
    "        input_size : int = 5\n",
    "        output_size : int = 20\n",
    "        num_channels : list = [80]*4\n",
    "        kernel_size : int =7\n",
    "        dropout : float = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "  'input_size' : 5,\n",
    "  'output_size' : 20,\n",
    "  'num_channels' : [80]*4,\n",
    "  'kernel_size' : 7,\n",
    "  'dropout' : 0.25\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trgs = TCNargumenets()\n",
    "\n",
    "\n",
    "from transformers import Trainer, TrainingArguments,HfArgumentParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataclass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ssu36/tiger/Stock_Prediction/models/TimeSeries/TCN/TCN_test.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/models/TimeSeries/TCN/TCN_test.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m@dataclass\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/models/TimeSeries/TCN/TCN_test.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mTCNargumenets\u001b[39;00m(HfArgumentParser):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/models/TimeSeries/TCN/TCN_test.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         input_size : \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/models/TimeSeries/TCN/TCN_test.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         output_size : \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataclass' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "@dataclass\n",
    "class TCNargumenets(HfArgumentParser):\n",
    "        input_size : int = 5\n",
    "        output_size : int = 20\n",
    "        num_channels : list = [80]*4\n",
    "        kernel_size : int =7\n",
    "        dropout : float = 0.25\n",
    "parser = HfArgumentParser(TCNargumenets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ssu36/tiger/Stock_Prediction/models/TimeSeries/TCN/TCN_test.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/models/TimeSeries/TCN/TCN_test.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m parse \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mparse_args()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parser' is not defined"
     ]
    }
   ],
   "source": [
    "parse = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssu36/anaconda3/envs/tiger/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "tcn_args = TCNargumenets()\n",
    "\n",
    "model = TCNModel(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TCNModel(\n",
       "  (tcn): TemporalConvNet(\n",
       "    (network): Sequential(\n",
       "      (0): TemporalBlock(\n",
       "        (conv1): Conv1d(5, 80, kernel_size=(7,), stride=(1,), padding=(6,))\n",
       "        (chomp1): Chomp1d()\n",
       "        (relu1): ReLU()\n",
       "        (dropout1): Dropout(p=0.25, inplace=False)\n",
       "        (conv2): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(6,))\n",
       "        (chomp2): Chomp1d()\n",
       "        (relu2): ReLU()\n",
       "        (dropout2): Dropout(p=0.25, inplace=False)\n",
       "        (net): Sequential(\n",
       "          (0): Conv1d(5, 80, kernel_size=(7,), stride=(1,), padding=(6,))\n",
       "          (1): Chomp1d()\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.25, inplace=False)\n",
       "          (4): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(6,))\n",
       "          (5): Chomp1d()\n",
       "          (6): ReLU()\n",
       "          (7): Dropout(p=0.25, inplace=False)\n",
       "        )\n",
       "        (downsample): Conv1d(5, 80, kernel_size=(1,), stride=(1,))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): TemporalBlock(\n",
       "        (conv1): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
       "        (chomp1): Chomp1d()\n",
       "        (relu1): ReLU()\n",
       "        (dropout1): Dropout(p=0.25, inplace=False)\n",
       "        (conv2): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
       "        (chomp2): Chomp1d()\n",
       "        (relu2): ReLU()\n",
       "        (dropout2): Dropout(p=0.25, inplace=False)\n",
       "        (net): Sequential(\n",
       "          (0): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
       "          (1): Chomp1d()\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.25, inplace=False)\n",
       "          (4): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
       "          (5): Chomp1d()\n",
       "          (6): ReLU()\n",
       "          (7): Dropout(p=0.25, inplace=False)\n",
       "        )\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (2): TemporalBlock(\n",
       "        (conv1): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
       "        (chomp1): Chomp1d()\n",
       "        (relu1): ReLU()\n",
       "        (dropout1): Dropout(p=0.25, inplace=False)\n",
       "        (conv2): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
       "        (chomp2): Chomp1d()\n",
       "        (relu2): ReLU()\n",
       "        (dropout2): Dropout(p=0.25, inplace=False)\n",
       "        (net): Sequential(\n",
       "          (0): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
       "          (1): Chomp1d()\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.25, inplace=False)\n",
       "          (4): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
       "          (5): Chomp1d()\n",
       "          (6): ReLU()\n",
       "          (7): Dropout(p=0.25, inplace=False)\n",
       "        )\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (3): TemporalBlock(\n",
       "        (conv1): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(48,), dilation=(8,))\n",
       "        (chomp1): Chomp1d()\n",
       "        (relu1): ReLU()\n",
       "        (dropout1): Dropout(p=0.25, inplace=False)\n",
       "        (conv2): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(48,), dilation=(8,))\n",
       "        (chomp2): Chomp1d()\n",
       "        (relu2): ReLU()\n",
       "        (dropout2): Dropout(p=0.25, inplace=False)\n",
       "        (net): Sequential(\n",
       "          (0): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(48,), dilation=(8,))\n",
       "          (1): Chomp1d()\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.25, inplace=False)\n",
       "          (4): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(48,), dilation=(8,))\n",
       "          (5): Chomp1d()\n",
       "          (6): ReLU()\n",
       "          (7): Dropout(p=0.25, inplace=False)\n",
       "        )\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=80, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn_args = TCNargumenets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --target TARGET [--env ENV]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --target\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssu36/anaconda3/envs/tiger/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "# 인자값을 받을 수 있는 인스턴스 생성\n",
    "parser = argparse.ArgumentParser(description='사용법 테스트입니다.')\n",
    "\n",
    "# 입력받을 인자값 등록\n",
    "parser.add_argument('--target', required=True, help='어느 것을 요구하냐')\n",
    "parser.add_argument('--env', required=False, default='dev', help='실행환경은 뭐냐')\n",
    "\n",
    "# 입력받은 인자값을 args에 저장 (type: namespace)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssu36/anaconda3/envs/tiger/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TCNModel(\n",
       "  (tcn): TemporalConvNet(\n",
       "    (network): Sequential(\n",
       "      (0): TemporalBlock(\n",
       "        (conv1): Conv1d(5, 80, kernel_size=(7,), stride=(1,), padding=(6,))\n",
       "        (chomp1): Chomp1d()\n",
       "        (relu1): ReLU()\n",
       "        (dropout1): Dropout(p=0.25, inplace=False)\n",
       "        (conv2): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(6,))\n",
       "        (chomp2): Chomp1d()\n",
       "        (relu2): ReLU()\n",
       "        (dropout2): Dropout(p=0.25, inplace=False)\n",
       "        (net): Sequential(\n",
       "          (0): Conv1d(5, 80, kernel_size=(7,), stride=(1,), padding=(6,))\n",
       "          (1): Chomp1d()\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.25, inplace=False)\n",
       "          (4): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(6,))\n",
       "          (5): Chomp1d()\n",
       "          (6): ReLU()\n",
       "          (7): Dropout(p=0.25, inplace=False)\n",
       "        )\n",
       "        (downsample): Conv1d(5, 80, kernel_size=(1,), stride=(1,))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): TemporalBlock(\n",
       "        (conv1): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
       "        (chomp1): Chomp1d()\n",
       "        (relu1): ReLU()\n",
       "        (dropout1): Dropout(p=0.25, inplace=False)\n",
       "        (conv2): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
       "        (chomp2): Chomp1d()\n",
       "        (relu2): ReLU()\n",
       "        (dropout2): Dropout(p=0.25, inplace=False)\n",
       "        (net): Sequential(\n",
       "          (0): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
       "          (1): Chomp1d()\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.25, inplace=False)\n",
       "          (4): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
       "          (5): Chomp1d()\n",
       "          (6): ReLU()\n",
       "          (7): Dropout(p=0.25, inplace=False)\n",
       "        )\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (2): TemporalBlock(\n",
       "        (conv1): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
       "        (chomp1): Chomp1d()\n",
       "        (relu1): ReLU()\n",
       "        (dropout1): Dropout(p=0.25, inplace=False)\n",
       "        (conv2): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
       "        (chomp2): Chomp1d()\n",
       "        (relu2): ReLU()\n",
       "        (dropout2): Dropout(p=0.25, inplace=False)\n",
       "        (net): Sequential(\n",
       "          (0): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
       "          (1): Chomp1d()\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.25, inplace=False)\n",
       "          (4): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
       "          (5): Chomp1d()\n",
       "          (6): ReLU()\n",
       "          (7): Dropout(p=0.25, inplace=False)\n",
       "        )\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (3): TemporalBlock(\n",
       "        (conv1): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(48,), dilation=(8,))\n",
       "        (chomp1): Chomp1d()\n",
       "        (relu1): ReLU()\n",
       "        (dropout1): Dropout(p=0.25, inplace=False)\n",
       "        (conv2): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(48,), dilation=(8,))\n",
       "        (chomp2): Chomp1d()\n",
       "        (relu2): ReLU()\n",
       "        (dropout2): Dropout(p=0.25, inplace=False)\n",
       "        (net): Sequential(\n",
       "          (0): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(48,), dilation=(8,))\n",
       "          (1): Chomp1d()\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.25, inplace=False)\n",
       "          (4): Conv1d(80, 80, kernel_size=(7,), stride=(1,), padding=(48,), dilation=(8,))\n",
       "          (5): Chomp1d()\n",
       "          (6): ReLU()\n",
       "          (7): Dropout(p=0.25, inplace=False)\n",
       "        )\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=80, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TCNModel(tcn_args.input_size, tcn_args.output_size, tcn_args.num_channels, tcn_args.kernel_size, tcn_args.dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
