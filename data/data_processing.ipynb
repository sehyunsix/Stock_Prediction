{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import data_processing\n",
    "import torch\n",
    "from torch.utils.data import Dataset ,DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libpython3.10.so.1.0: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/sdfg8931/Stock_Prediction/data/data_processing.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2254505576332d382d41227d/home/sdfg8931/Stock_Prediction/data/data_processing.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch_xla\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/stock/lib/python3.10/site-packages/torch_xla/__init__.py:144\u001b[0m\n\u001b[1;32m    140\u001b[0m logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    141\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mLetting libtpu.so load fail during _XLAC import. libtpu.so will be loaded \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    142\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mfrom `libtpu` Python package when the ComputationClient is created.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    143\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mTPU_LOAD_LIBRARY\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 144\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39m_XLAC\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[39mdel\u001b[39;00m os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mTPU_LOAD_LIBRARY\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    147\u001b[0m _found_libtpu \u001b[39m=\u001b[39m _setup_tpu_vm_library_path()\n",
      "\u001b[0;31mImportError\u001b[0m: libpython3.10.so.1.0: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import torch_xla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://storage.googleapis.com/libtpu-releases/index.html\n",
      "Collecting torch~=2.1.0\n",
      "  Downloading torch-2.1.1-cp38-cp38-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting torch_xla~=2.1.0 (from torch_xla[tpu]~=2.1.0)\n",
      "  Downloading torch_xla-2.1.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.16.1-cp38-cp38-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting filelock (from torch~=2.1.0)\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/sdfg8931/miniconda/envs/main/lib/python3.8/site-packages (from torch~=2.1.0) (4.8.0)\n",
      "Collecting sympy (from torch~=2.1.0)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx (from torch~=2.1.0)\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jinja2 (from torch~=2.1.0)\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting fsspec (from torch~=2.1.0)\n",
      "  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch~=2.1.0)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch~=2.1.0)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch~=2.1.0)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch~=2.1.0)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch~=2.1.0)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch~=2.1.0)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch~=2.1.0)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch~=2.1.0)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch~=2.1.0)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch~=2.1.0)\n",
      "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch~=2.1.0)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting triton==2.1.0 (from torch~=2.1.0)\n",
      "  Downloading triton-2.1.0-0-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.0)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting absl-py>=1.0.0 (from torch_xla~=2.1.0->torch_xla[tpu]~=2.1.0)\n",
      "  Using cached absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting cloud-tpu-client>=0.10.0 (from torch_xla~=2.1.0->torch_xla[tpu]~=2.1.0)\n",
      "  Using cached cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
      "Collecting pyyaml (from torch_xla~=2.1.0->torch_xla[tpu]~=2.1.0)\n",
      "  Downloading PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting requests (from torchvision)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Downloading Pillow-10.1.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting libtpu-nightly==0.1.dev20230825 (from torch_xla[tpu]~=2.1.0)\n",
      "  Using cached https://storage.googleapis.com/libtpu-default-releases/wheels/libtpu-nightly/libtpu_nightly-0.1.dev20230825%2Bdefault-py3-none-any.whl (91.8 MB)\n",
      "Collecting google-api-python-client==1.8.0 (from cloud-tpu-client>=0.10.0->torch_xla~=2.1.0->torch_xla[tpu]~=2.1.0)\n",
      "  Using cached google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
      "Collecting oauth2client (from cloud-tpu-client>=0.10.0->torch_xla~=2.1.0->torch_xla[tpu]~=2.1.0)\n",
      "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Collecting httplib2<1dev,>=0.9.2 (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.1.0->torch_xla[tpu]~=2.1.0)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Collecting google-auth>=1.4.1 (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.1.0->torch_xla[tpu]~=2.1.0)\n",
      "  Using cached google_auth-2.23.4-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-httplib2>=0.0.3 (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.1.0->torch_xla[tpu]~=2.1.0)\n",
      "  Using cached google_auth_httplib2-0.1.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting google-api-core<2dev,>=1.13.0 (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.1.0->torch_xla[tpu]~=2.1.0)\n",
      "  Using cached google_api_core-1.34.0-py3-none-any.whl (120 kB)\n",
      "Requirement already satisfied: six<2dev,>=1.6.1 in /home/sdfg8931/miniconda/envs/main/lib/python3.8/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.1.0->torch_xla[tpu]~=2.1.0) (1.16.0)\n",
      "Collecting uritemplate<4dev,>=3.0.0 (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.1.0->torch_xla[tpu]~=2.1.0)\n",
      "  Using cached uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch~=2.1.0)\n",
      "  Downloading MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->torchvision)\n",
      "  Downloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->torchvision)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->torchvision)\n",
      "  Using cached urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->torchvision)\n",
      "  Using cached certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch~=2.1.0)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting pyasn1>=0.1.7 (from oauth2client->cloud-tpu-client>=0.10.0->torch_xla~=2.1.0->torch_xla[tpu]~=2.1.0)\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pyasn1-modules>=0.0.5 (from oauth2client->cloud-tpu-client>=0.10.0->torch_xla~=2.1.0->torch_xla[tpu]~=2.1.0)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa>=3.1.4 (from oauth2client->cloud-tpu-client>=0.10.0->torch_xla~=2.1.0->torch_xla[tpu]~=2.1.0)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.56.2 (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.1.0->torch_xla[tpu]~=2.1.0)\n",
      "  Using cached googleapis_common_protos-1.61.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.1.0->torch_xla[tpu]~=2.1.0)\n",
      "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0 (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.1.0->torch_xla[tpu]~=2.1.0)\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch_xla~=2.1.0->torch_xla[tpu]~=2.1.0)\n",
      "  Using cached pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Downloading torch-2.1.1-cp38-cp38-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Downloading triton-2.1.0-0-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch_xla-2.1.0-cp38-cp38-manylinux_2_28_x86_64.whl (81.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading torchvision-0.16.1-cp38-cp38-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "Downloading Pillow-10.1.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Using cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (736 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m736.6/736.6 kB\u001b[0m \u001b[31m986.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "Downloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m220.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Downloading MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "Using cached google_auth-2.23.4-py2.py3-none-any.whl (183 kB)\n",
      "Using cached google_auth_httplib2-0.1.1-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Using cached googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
      "Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Installing collected packages: mpmath, libtpu-nightly, urllib3, uritemplate, sympy, pyyaml, pyparsing, pyasn1, protobuf, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, cachetools, absl-py, triton, rsa, requests, pyasn1-modules, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, httplib2, googleapis-common-protos, oauth2client, nvidia-cusolver-cu12, google-auth, torch, google-auth-httplib2, google-api-core, torchvision, google-api-python-client, cloud-tpu-client, torch_xla\n",
      "Successfully installed MarkupSafe-2.1.3 absl-py-2.0.0 cachetools-5.3.2 certifi-2023.11.17 charset-normalizer-3.3.2 cloud-tpu-client-0.10 filelock-3.13.1 fsspec-2023.10.0 google-api-core-1.34.0 google-api-python-client-1.8.0 google-auth-2.23.4 google-auth-httplib2-0.1.1 googleapis-common-protos-1.61.0 httplib2-0.22.0 idna-3.6 jinja2-3.1.2 libtpu-nightly-0.1.dev20230825+default mpmath-1.3.0 networkx-3.1 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 oauth2client-4.1.3 pillow-10.1.0 protobuf-3.20.3 pyasn1-0.5.1 pyasn1-modules-0.3.0 pyparsing-3.1.1 pyyaml-6.0.1 requests-2.31.0 rsa-4.9 sympy-1.12 torch-2.1.1 torch_xla-2.1.0 torchvision-0.16.1 triton-2.1.0 uritemplate-3.0.1 urllib3-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch~=2.1.0 torch_xla[tpu]~=2.1.0 torchvision -f https://storage.googleapis.com/libtpu-releases/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "max_len = 5000\n",
    "d_model=5\n",
    "pe = torch.zeros(5000, d_model)\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe[:, 0::2] = torch.sin(position * div_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe[:, 1::2] = torch.sin(position * div_term[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_term.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_term.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('NASDAQ_3y/stock_price.csv')\n",
    "# data.columns = ['Date', 'Ticker', 'Open', 'High','Low', 'Close', 'Volume','sell','buy' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['Date', 'Ticker', 'Open', 'High','Low', 'Close', 'Volume','sell','buy' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.groupby('Ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_day_list =[]\n",
    "for group in data:\n",
    "  group_day_list.append(len(group[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({755: 2136, 724: 15, 605: 10, 671: 10, 754: 8, 604: 7, 511: 7, 705: 7, 584: 7, 608: 6, 520: 6, 546: 6, 649: 6, 527: 6, 589: 6, 552: 6, 742: 5, 676: 5, 711: 5, 613: 5, 609: 5, 703: 5, 652: 5, 659: 4, 667: 4, 744: 4, 678: 4, 738: 4, 541: 4, 526: 4, 619: 4, 727: 4, 585: 4, 734: 4, 313: 4, 743: 3, 590: 3, 655: 3, 721: 3, 600: 3, 522: 3, 657: 3, 472: 3, 658: 3, 617: 3, 532: 3, 739: 3, 512: 3, 594: 3, 702: 3, 507: 3, 618: 3, 544: 3, 736: 3, 574: 3, 637: 3, 665: 3, 748: 3, 448: 3, 718: 3, 690: 3, 660: 3, 645: 3, 708: 3, 715: 3, 723: 3, 515: 3, 614: 3, 681: 3, 635: 3, 591: 3, 700: 3, 445: 3, 749: 3, 747: 3, 745: 3, 595: 3, 728: 3, 513: 3, 753: 2, 624: 2, 668: 2, 453: 2, 400: 2, 257: 2, 673: 2, 304: 2, 427: 2, 362: 2, 516: 2, 687: 2, 641: 2, 698: 2, 383: 2, 517: 2, 395: 2, 701: 2, 587: 2, 717: 2, 642: 2, 686: 2, 467: 2, 535: 2, 493: 2, 737: 2, 227: 2, 672: 2, 459: 2, 621: 2, 628: 2, 1: 2, 586: 2, 476: 2, 489: 2, 540: 2, 713: 2, 521: 2, 629: 2, 547: 2, 323: 2, 639: 2, 750: 2, 531: 2, 492: 2, 579: 2, 647: 2, 449: 2, 565: 2, 495: 2, 519: 2, 301: 2, 322: 2, 333: 2, 553: 2, 455: 2, 710: 2, 653: 2, 740: 2, 397: 2, 654: 2, 714: 2, 709: 2, 706: 2, 680: 2, 525: 2, 385: 1, 551: 1, 683: 1, 484: 1, 650: 1, 616: 1, 72: 1, 543: 1, 419: 1, 406: 1, 452: 1, 260: 1, 315: 1, 259: 1, 230: 1, 732: 1, 374: 1, 309: 1, 330: 1, 443: 1, 465: 1, 634: 1, 704: 1, 666: 1, 627: 1, 731: 1, 651: 1, 510: 1, 567: 1, 685: 1, 675: 1, 670: 1, 334: 1, 391: 1, 422: 1, 580: 1, 610: 1, 461: 1, 340: 1, 640: 1, 688: 1, 319: 1, 236: 1, 542: 1, 538: 1, 611: 1, 359: 1, 469: 1, 545: 1, 311: 1, 539: 1, 741: 1, 677: 1, 693: 1, 596: 1, 345: 1, 487: 1, 358: 1, 353: 1, 240: 1, 581: 1, 289: 1, 722: 1, 505: 1, 112: 1, 363: 1, 293: 1, 271: 1, 328: 1, 502: 1, 689: 1, 314: 1, 674: 1, 620: 1, 549: 1, 413: 1, 694: 1, 483: 1, 299: 1, 499: 1, 631: 1, 633: 1, 372: 1, 692: 1, 390: 1, 368: 1, 697: 1, 275: 1, 237: 1, 669: 1, 578: 1, 663: 1, 415: 1, 241: 1, 607: 1, 446: 1, 569: 1, 310: 1, 270: 1, 601: 1, 396: 1, 563: 1, 564: 1, 575: 1, 316: 1, 405: 1, 371: 1, 684: 1, 324: 1, 682: 1, 536: 1, 466: 1, 555: 1, 251: 1, 468: 1, 598: 1, 350: 1, 662: 1, 537: 1, 632: 1, 339: 1, 341: 1, 615: 1, 302: 1, 720: 1, 414: 1, 725: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "group_day_counts = Counter(group_day_list)\n",
    "print(group_day_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE  = 90\n",
    "PREDICT_SIZE = 30\n",
    "SLIDING_SIZE =1\n",
    "FEATURE_SIZE = 6\n",
    "HIDDEN_SIZE =40\n",
    "BATCH_SIZE =64\n",
    "FULLLY_SIZE =128\n",
    "FEATURE = [ 'Open', 'High','Low', 'Volume','sell','buy' ]\n",
    "TARGET = 'Close'\n",
    "TICKER_NUMBER =2743\n",
    "TOTAL_DAY = 166\n",
    "WINDOW_NUMBER =int(TOTAL_DAY - (WINDOW_SIZE+PREDICT_SIZE)/SLIDING_SIZE+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def min_max(sequences):\n",
    "  results = sequences.copy()\n",
    "  v_min =results.min()\n",
    "  v_max =results.max()\n",
    "  new_min =0\n",
    "  new_max =1\n",
    "  min_max=[]\n",
    "  for index,sequence in enumerate(results):\n",
    "    v_min =sequence.min()\n",
    "    v_max =sequence.max()\n",
    "    v_p = (sequence - v_min)/(v_max - v_min)*(new_max - new_min) + new_min\n",
    "    min_max.append([v_min,v_max])\n",
    "    results[index] = v_p\n",
    "  return results, min_max\n",
    "\n",
    "def inverse_min_max(sequences,min_max):\n",
    "  original = []\n",
    "  for index,sequence in enumerate(sequences):\n",
    "    v_min =min_max[index][0]\n",
    "    v_max =min_max[index][1]\n",
    "    sequence_restored = (sequence) * (v_max - v_min) + v_min\n",
    "    original.append(sequence_restored)\n",
    "  return np.array(original)\n",
    "\n",
    "def preprocess_lstm(df):\n",
    "  sequences = list()\n",
    "  for group in df.groupby('Ticker'):\n",
    "    sequences.append(group[1][FEATURE])\n",
    "  sequences=np.array(sequences)\n",
    "\n",
    "  ## min_max trading\n",
    "  price , trade,sentiments =np.split(sequences,[4,5],axis=2)\n",
    "  trade , min_max_trading =min_max(trade)\n",
    "  price , min_max_list =min_max(price)\n",
    "  combine =np.concatenate([price,trade,sentiments],axis =2)\n",
    "\n",
    "  ## min_max another\n",
    "\n",
    "  ## min_max another\n",
    "  result_list= []\n",
    "  for i in range(0,WINDOW_NUMBER):\n",
    "    a, b, c= np.split(combine,[i,i+120],axis=1)\n",
    "    result_list.append(b)\n",
    "  result_array = np.array(result_list)\n",
    "  train ,vaild = np.split(result_array,[46],axis=0)\n",
    "  train_x,train_y=np.split(train.reshape(46*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "  vaild_x,vaild_y=np.split(vaild.reshape(1*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "  return train_x,train_y,vaild_x,vaild_y,min_max_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,train_y,vaild_x,vaild_y,min_max_list = preprocess_lstm(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, target):\n",
    "        self.x_data = torch.tensor(x_data,dtype =torch.float32)\n",
    "        self.y_data = torch.tensor(y_data[:,:,FEATURE.index(target)],dtype = torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 105989520 into shape (126178,120,6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m   vaild_x,vaild_y\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39msplit(vaild\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m\u001b[39m*\u001b[39mTICKER_NUMBER,WINDOW_SIZE\u001b[39m+\u001b[39mPREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m train_x,train_y,vaild_x,vaild_y,min_max_list\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m train_x,train_y,vaild_x,vaild_y,min_max_list \u001b[39m=\u001b[39m preprocess_lstm(data)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m train_x\u001b[39m.\u001b[39mshape,train_y\u001b[39m.\u001b[39mshape,vaild_x\u001b[39m.\u001b[39mshape,vaild_y\u001b[39m.\u001b[39mshape,\u001b[39mlen\u001b[39m(min_max_list)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBaseDataset\u001b[39;00m(Dataset):\n",
      "\u001b[1;32m/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m result_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(result_list)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m train ,vaild \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msplit(result_array,[\u001b[39m46\u001b[39m],axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m train_x,train_y\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39msplit(train\u001b[39m.\u001b[39;49mreshape(\u001b[39m46\u001b[39;49m\u001b[39m*\u001b[39;49mTICKER_NUMBER,WINDOW_SIZE\u001b[39m+\u001b[39;49mPREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m vaild_x,vaild_y\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39msplit(vaild\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m\u001b[39m*\u001b[39mTICKER_NUMBER,WINDOW_SIZE\u001b[39m+\u001b[39mPREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mreturn\u001b[39;00m train_x,train_y,vaild_x,vaild_y,min_max_list\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 105989520 into shape (126178,120,6)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data= pd.read_csv('stockPrice/NASDAQ.csv')\n",
    "data.columns = ['Date', 'Ticker', 'Open', 'High','Low', 'Close', 'Volume','sell','buy' ]\n",
    "\n",
    "\n",
    "\n",
    "def min_max(sequences):\n",
    "  results = sequences.copy()\n",
    "  v_min =results.min()\n",
    "  v_max =results.max()\n",
    "  new_min =0\n",
    "  new_max =1\n",
    "  min_max=[]\n",
    "  for index,sequence in enumerate(results):\n",
    "    v_min =sequence.min()\n",
    "    v_max =sequence.max()\n",
    "    v_p = (sequence - v_min)/(v_max - v_min)*(new_max - new_min) + new_min\n",
    "    min_max.append([v_min,v_max])\n",
    "    results[index] = v_p\n",
    "  return results, min_max\n",
    "\n",
    "def inverse_min_max(sequences,min_max):\n",
    "  original = []\n",
    "  for index,sequence in enumerate(sequences):\n",
    "    v_min =min_max[index][0]\n",
    "    v_max =min_max[index][1]\n",
    "    sequence_restored = (sequence) * (v_max - v_min) + v_min\n",
    "    original.append(sequence_restored)\n",
    "  return np.array(original)\n",
    "\n",
    "\n",
    "def make_split(sequences,WINDOW_NUMBER,PREDICT_SIZE,WINDOW_SIZE,FEATURE_SIZE,TICKER_NUMBER,TOTAL_DAY):\n",
    "  for i in range(0,WINDOW_NUMBER):\n",
    "    a, b, c= np.split(sequences,[i,i+120],axis=1)\n",
    "    original_list.append(b)\n",
    "  result_array = np.array(result_list)\n",
    "  train ,vaild = np.split(result_array,[46],axis=0)\n",
    "  train_x,train_y=np.split(train.reshape((TOTAL_DAY-1)*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "  vaild_x,vaild_y=np.split(vaild.reshape(1*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "\n",
    "\n",
    "def preprocess_lstm(df):\n",
    "  sequences = list()\n",
    "  for group in df.groupby('Ticker'):\n",
    "    sequences.append(group[1][FEATURE])\n",
    "  sequences=np.array(sequences)\n",
    "\n",
    "  ## min_max trading\n",
    "  price , trade,sentiments =np.split(sequences,[4,5],axis=2)\n",
    "  trade , min_max_trading =min_max(trade)\n",
    "  price , min_max_list =min_max(price)\n",
    "  combine =np.concatenate([price,trade,sentiments],axis =2)\n",
    "\n",
    "  ## min_max another\n",
    "  original_list= []\n",
    "  for i in range(0,WINDOW_NUMBER):\n",
    "    a, b, c= np.split(sequences,[i,i+120],axis=1)\n",
    "    original_list.append(b)\n",
    "  result_array = np.array(result_list)\n",
    "  train ,vaild = np.split(result_array,[46],axis=0)\n",
    "  train_x,train_y=np.split(train.reshape(46*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "  vaild_x,vaild_y=np.split(vaild.reshape(1*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "\n",
    "  ## min_max another\n",
    "  result_list= []\n",
    "  for i in range(0,WINDOW_NUMBER):\n",
    "    a, b, c= np.split(combine,[i,i+120],axis=1)\n",
    "    result_list.append(b)\n",
    "  result_array = np.array(result_list)\n",
    "  train ,vaild = np.split(result_array,[46],axis=0)\n",
    "\n",
    "  train_x,train_y=np.split(train.reshape(46*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "  vaild_x,vaild_y=np.split(vaild.reshape(1*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "  return train_x,train_y,vaild_x,vaild_y,min_max_list\n",
    "\n",
    "train_x,train_y,vaild_x,vaild_y,min_max_list = preprocess_lstm(data)\n",
    "train_x.shape,train_y.shape,vaild_x.shape,vaild_y.shape,len(min_max_list)\n",
    "\n",
    "\n",
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, target):\n",
    "        self.x_data = torch.tensor(x_data,dtype =torch.float32)\n",
    "        self.y_data = torch.tensor(y_data[:,:,FEATURE.index(target)],dtype = torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x_data.shape[0]\n",
    "\n",
    "train_data = BaseDataset(train_x,train_y,TARGET)\n",
    "test_data =  BaseDataset(vaild_x,vaild_y,TARGET)\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_data \u001b[39m=\u001b[39m BaseDataset(train_x,train_y,TARGET)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m test_data \u001b[39m=\u001b[39m  BaseDataset(vaild_x,vaild_y,TARGET)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(train_data, batch_size\u001b[39m=\u001b[39mBATCH_SIZE, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseDataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_data = BaseDataset(train_x,train_y,TARGET)\n",
    "test_data =  BaseDataset(vaild_x,vaild_y,TARGET)\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orginal5 = torch.reshape(train_data.x_data[0],(,90,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Window_maker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m Window_maker\u001b[39m.\u001b[39minverse_min_max(original[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Window_maker' is not defined"
     ]
    }
   ],
   "source": [
    "Window_maker.inverse_min_max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'original' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m torch\u001b[39m.\u001b[39mtensor(original[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'original' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.tensor(original[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_x,train_y,vaild_x,vaild_y,min_max_list = preprocess_lstm(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def min_max(sequences):\n",
    "  results = sequences.copy()\n",
    "  v_min =results.min()\n",
    "  v_max =results.max()\n",
    "  new_min =0\n",
    "  new_max =1\n",
    "  min_max=[]\n",
    "  for index,sequence in enumerate(results):\n",
    "    v_min =sequence.min()\n",
    "    v_max =sequence.max()\n",
    "    v_p = (sequence - v_min)/(v_max - v_min)*(new_max - new_min) + new_min\n",
    "    min_max.append([v_min,v_max])\n",
    "    results[index] = v_p\n",
    "  return results, min_max\n",
    "\n",
    "def inverse_min_max(sequences,min_max):\n",
    "  original = []\n",
    "  for index,sequence in enumerate(sequences):\n",
    "    v_min =min_max[index][0]\n",
    "    v_max =min_max[index][1]\n",
    "    sequence_restored = (sequence) * (v_max - v_min) + v_min\n",
    "    original.append(sequence_restored)\n",
    "  return np.array(original)\n",
    "\n",
    "def preprocess_lstm(df):\n",
    "  sequences = list()\n",
    "  for group in df.groupby('Ticker'):\n",
    "    sequences.append(group[1][FEATURE])\n",
    "  sequences=np.array(sequences)\n",
    "\n",
    "  ## min_max trading\n",
    "  price , trade,sentiments =np.split(sequences,[4,5],axis=2)\n",
    "  trade , min_max_trading =min_max(trade)\n",
    "  price , min_max_list =min_max(price)\n",
    "  combine =np.concatenate([price,trade,sentiments],axis =2)\n",
    "\n",
    "  ## min_max another\n",
    "\n",
    "  ## min_max another\n",
    "  result_list= []\n",
    "  for i in range(0,WINDOW_NUMBER):\n",
    "    a, b, c= np.split(combine,[i,i+120],axis=1)\n",
    "    result_list.append(b)\n",
    "  result_array = np.array(result_list)\n",
    "  train ,vaild = np.split(result_array,[46],axis=0)\n",
    "  train_x,train_y=np.split(train.reshape(46*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "  vaild_x,vaild_y=np.split(vaild.reshape(1*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "  return train_x,train_y,vaild_x,vaild_y,min_max_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
