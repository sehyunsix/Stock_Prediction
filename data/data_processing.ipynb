{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import data_processing\n",
    "import torch\n",
    "from torch.utils.data import Dataset ,DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('NASDAQ_3y/stock_price.csv')\n",
    "# data.columns = ['Date', 'Ticker', 'Open', 'High','Low', 'Close', 'Volume','sell','buy' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['Date', 'Ticker', 'Open', 'High','Low', 'Close', 'Volume','sell','buy' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.groupby('Ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_day_list =[]\n",
    "for group in data:\n",
    "  group_day_list.append(len(group[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({755: 2136, 724: 15, 605: 10, 671: 10, 754: 8, 604: 7, 511: 7, 705: 7, 584: 7, 608: 6, 520: 6, 546: 6, 649: 6, 527: 6, 589: 6, 552: 6, 742: 5, 676: 5, 711: 5, 613: 5, 609: 5, 703: 5, 652: 5, 659: 4, 667: 4, 744: 4, 678: 4, 738: 4, 541: 4, 526: 4, 619: 4, 727: 4, 585: 4, 734: 4, 313: 4, 743: 3, 590: 3, 655: 3, 721: 3, 600: 3, 522: 3, 657: 3, 472: 3, 658: 3, 617: 3, 532: 3, 739: 3, 512: 3, 594: 3, 702: 3, 507: 3, 618: 3, 544: 3, 736: 3, 574: 3, 637: 3, 665: 3, 748: 3, 448: 3, 718: 3, 690: 3, 660: 3, 645: 3, 708: 3, 715: 3, 723: 3, 515: 3, 614: 3, 681: 3, 635: 3, 591: 3, 700: 3, 445: 3, 749: 3, 747: 3, 745: 3, 595: 3, 728: 3, 513: 3, 753: 2, 624: 2, 668: 2, 453: 2, 400: 2, 257: 2, 673: 2, 304: 2, 427: 2, 362: 2, 516: 2, 687: 2, 641: 2, 698: 2, 383: 2, 517: 2, 395: 2, 701: 2, 587: 2, 717: 2, 642: 2, 686: 2, 467: 2, 535: 2, 493: 2, 737: 2, 227: 2, 672: 2, 459: 2, 621: 2, 628: 2, 1: 2, 586: 2, 476: 2, 489: 2, 540: 2, 713: 2, 521: 2, 629: 2, 547: 2, 323: 2, 639: 2, 750: 2, 531: 2, 492: 2, 579: 2, 647: 2, 449: 2, 565: 2, 495: 2, 519: 2, 301: 2, 322: 2, 333: 2, 553: 2, 455: 2, 710: 2, 653: 2, 740: 2, 397: 2, 654: 2, 714: 2, 709: 2, 706: 2, 680: 2, 525: 2, 385: 1, 551: 1, 683: 1, 484: 1, 650: 1, 616: 1, 72: 1, 543: 1, 419: 1, 406: 1, 452: 1, 260: 1, 315: 1, 259: 1, 230: 1, 732: 1, 374: 1, 309: 1, 330: 1, 443: 1, 465: 1, 634: 1, 704: 1, 666: 1, 627: 1, 731: 1, 651: 1, 510: 1, 567: 1, 685: 1, 675: 1, 670: 1, 334: 1, 391: 1, 422: 1, 580: 1, 610: 1, 461: 1, 340: 1, 640: 1, 688: 1, 319: 1, 236: 1, 542: 1, 538: 1, 611: 1, 359: 1, 469: 1, 545: 1, 311: 1, 539: 1, 741: 1, 677: 1, 693: 1, 596: 1, 345: 1, 487: 1, 358: 1, 353: 1, 240: 1, 581: 1, 289: 1, 722: 1, 505: 1, 112: 1, 363: 1, 293: 1, 271: 1, 328: 1, 502: 1, 689: 1, 314: 1, 674: 1, 620: 1, 549: 1, 413: 1, 694: 1, 483: 1, 299: 1, 499: 1, 631: 1, 633: 1, 372: 1, 692: 1, 390: 1, 368: 1, 697: 1, 275: 1, 237: 1, 669: 1, 578: 1, 663: 1, 415: 1, 241: 1, 607: 1, 446: 1, 569: 1, 310: 1, 270: 1, 601: 1, 396: 1, 563: 1, 564: 1, 575: 1, 316: 1, 405: 1, 371: 1, 684: 1, 324: 1, 682: 1, 536: 1, 466: 1, 555: 1, 251: 1, 468: 1, 598: 1, 350: 1, 662: 1, 537: 1, 632: 1, 339: 1, 341: 1, 615: 1, 302: 1, 720: 1, 414: 1, 725: 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "group_day_counts = Counter(group_day_list)\n",
    "print(group_day_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE  = 90\n",
    "PREDICT_SIZE = 30\n",
    "SLIDING_SIZE =1\n",
    "FEATURE_SIZE = 6\n",
    "HIDDEN_SIZE =40\n",
    "BATCH_SIZE =64\n",
    "FULLLY_SIZE =128\n",
    "FEATURE = [ 'Open', 'High','Low', 'Volume','sell','buy' ]\n",
    "TARGET = 'Close'\n",
    "TICKER_NUMBER =2743\n",
    "TOTAL_DAY = 166\n",
    "WINDOW_NUMBER =int(TOTAL_DAY - (WINDOW_SIZE+PREDICT_SIZE)/SLIDING_SIZE+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def min_max(sequences):\n",
    "  results = sequences.copy()\n",
    "  v_min =results.min()\n",
    "  v_max =results.max()\n",
    "  new_min =0\n",
    "  new_max =1\n",
    "  min_max=[]\n",
    "  for index,sequence in enumerate(results):\n",
    "    v_min =sequence.min()\n",
    "    v_max =sequence.max()\n",
    "    v_p = (sequence - v_min)/(v_max - v_min)*(new_max - new_min) + new_min\n",
    "    min_max.append([v_min,v_max])\n",
    "    results[index] = v_p\n",
    "  return results, min_max\n",
    "\n",
    "def inverse_min_max(sequences,min_max):\n",
    "  original = []\n",
    "  for index,sequence in enumerate(sequences):\n",
    "    v_min =min_max[index][0]\n",
    "    v_max =min_max[index][1]\n",
    "    sequence_restored = (sequence) * (v_max - v_min) + v_min\n",
    "    original.append(sequence_restored)\n",
    "  return np.array(original)\n",
    "\n",
    "def preprocess_lstm(df):\n",
    "  sequences = list()\n",
    "  for group in df.groupby('Ticker'):\n",
    "    sequences.append(group[1][FEATURE])\n",
    "  sequences=np.array(sequences)\n",
    "\n",
    "  ## min_max trading\n",
    "  price , trade,sentiments =np.split(sequences,[4,5],axis=2)\n",
    "  trade , min_max_trading =min_max(trade)\n",
    "  price , min_max_list =min_max(price)\n",
    "  combine =np.concatenate([price,trade,sentiments],axis =2)\n",
    "\n",
    "  ## min_max another\n",
    "\n",
    "  ## min_max another\n",
    "  result_list= []\n",
    "  for i in range(0,WINDOW_NUMBER):\n",
    "    a, b, c= np.split(combine,[i,i+120],axis=1)\n",
    "    result_list.append(b)\n",
    "  result_array = np.array(result_list)\n",
    "  train ,vaild = np.split(result_array,[46],axis=0)\n",
    "  train_x,train_y=np.split(train.reshape(46*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "  vaild_x,vaild_y=np.split(vaild.reshape(1*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "  return train_x,train_y,vaild_x,vaild_y,min_max_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,train_y,vaild_x,vaild_y,min_max_list = preprocess_lstm(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, target):\n",
    "        self.x_data = torch.tensor(x_data,dtype =torch.float32)\n",
    "        self.y_data = torch.tensor(y_data[:,:,FEATURE.index(target)],dtype = torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 105989520 into shape (126178,120,6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m   vaild_x,vaild_y\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39msplit(vaild\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m\u001b[39m*\u001b[39mTICKER_NUMBER,WINDOW_SIZE\u001b[39m+\u001b[39mPREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m train_x,train_y,vaild_x,vaild_y,min_max_list\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m train_x,train_y,vaild_x,vaild_y,min_max_list \u001b[39m=\u001b[39m preprocess_lstm(data)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m train_x\u001b[39m.\u001b[39mshape,train_y\u001b[39m.\u001b[39mshape,vaild_x\u001b[39m.\u001b[39mshape,vaild_y\u001b[39m.\u001b[39mshape,\u001b[39mlen\u001b[39m(min_max_list)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBaseDataset\u001b[39;00m(Dataset):\n",
      "\u001b[1;32m/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m result_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(result_list)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m train ,vaild \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msplit(result_array,[\u001b[39m46\u001b[39m],axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m train_x,train_y\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39msplit(train\u001b[39m.\u001b[39;49mreshape(\u001b[39m46\u001b[39;49m\u001b[39m*\u001b[39;49mTICKER_NUMBER,WINDOW_SIZE\u001b[39m+\u001b[39;49mPREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m vaild_x,vaild_y\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39msplit(vaild\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m\u001b[39m*\u001b[39mTICKER_NUMBER,WINDOW_SIZE\u001b[39m+\u001b[39mPREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mreturn\u001b[39;00m train_x,train_y,vaild_x,vaild_y,min_max_list\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 105989520 into shape (126178,120,6)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data= pd.read_csv('stockPrice/NASDAQ.csv')\n",
    "data.columns = ['Date', 'Ticker', 'Open', 'High','Low', 'Close', 'Volume','sell','buy' ]\n",
    "\n",
    "\n",
    "\n",
    "def min_max(sequences):\n",
    "  results = sequences.copy()\n",
    "  v_min =results.min()\n",
    "  v_max =results.max()\n",
    "  new_min =0\n",
    "  new_max =1\n",
    "  min_max=[]\n",
    "  for index,sequence in enumerate(results):\n",
    "    v_min =sequence.min()\n",
    "    v_max =sequence.max()\n",
    "    v_p = (sequence - v_min)/(v_max - v_min)*(new_max - new_min) + new_min\n",
    "    min_max.append([v_min,v_max])\n",
    "    results[index] = v_p\n",
    "  return results, min_max\n",
    "\n",
    "def inverse_min_max(sequences,min_max):\n",
    "  original = []\n",
    "  for index,sequence in enumerate(sequences):\n",
    "    v_min =min_max[index][0]\n",
    "    v_max =min_max[index][1]\n",
    "    sequence_restored = (sequence) * (v_max - v_min) + v_min\n",
    "    original.append(sequence_restored)\n",
    "  return np.array(original)\n",
    "\n",
    "def preprocess_lstm(df):\n",
    "  sequences = list()\n",
    "  for group in df.groupby('Ticker'):\n",
    "    sequences.append(group[1][FEATURE])\n",
    "  sequences=np.array(sequences)\n",
    "\n",
    "  ## min_max trading\n",
    "  price , trade,sentiments =np.split(sequences,[4,5],axis=2)\n",
    "  trade , min_max_trading =min_max(trade)\n",
    "  price , min_max_list =min_max(price)\n",
    "  combine =np.concatenate([price,trade,sentiments],axis =2)\n",
    "\n",
    "  ## min_max another\n",
    "\n",
    "  ## min_max another\n",
    "  result_list= []\n",
    "  for i in range(0,WINDOW_NUMBER):\n",
    "    a, b, c= np.split(combine,[i,i+120],axis=1)\n",
    "    result_list.append(b)\n",
    "  result_array = np.array(result_list)\n",
    "  train ,vaild = np.split(result_array,[46],axis=0)\n",
    "  train_x,train_y=np.split(train.reshape(46*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "  vaild_x,vaild_y=np.split(vaild.reshape(1*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "  return train_x,train_y,vaild_x,vaild_y,min_max_list\n",
    "\n",
    "train_x,train_y,vaild_x,vaild_y,min_max_list = preprocess_lstm(data)\n",
    "train_x.shape,train_y.shape,vaild_x.shape,vaild_y.shape,len(min_max_list)\n",
    "\n",
    "\n",
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, target):\n",
    "        self.x_data = torch.tensor(x_data,dtype =torch.float32)\n",
    "        self.y_data = torch.tensor(y_data[:,:,FEATURE.index(target)],dtype = torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x_data.shape[0]\n",
    "\n",
    "train_data = BaseDataset(train_x,train_y,TARGET)\n",
    "test_data =  BaseDataset(vaild_x,vaild_y,TARGET)\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_data \u001b[39m=\u001b[39m BaseDataset(train_x,train_y,TARGET)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m test_data \u001b[39m=\u001b[39m  BaseDataset(vaild_x,vaild_y,TARGET)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.253.25.176/home/ssu36/tiger/Stock_Prediction/data/data_processing.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(train_data, batch_size\u001b[39m=\u001b[39mBATCH_SIZE, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseDataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_data = BaseDataset(train_x,train_y,TARGET)\n",
    "test_data =  BaseDataset(vaild_x,vaild_y,TARGET)\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_x,train_y,vaild_x,vaild_y,min_max_list = preprocess_lstm(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def min_max(sequences):\n",
    "  results = sequences.copy()\n",
    "  v_min =results.min()\n",
    "  v_max =results.max()\n",
    "  new_min =0\n",
    "  new_max =1\n",
    "  min_max=[]\n",
    "  for index,sequence in enumerate(results):\n",
    "    v_min =sequence.min()\n",
    "    v_max =sequence.max()\n",
    "    v_p = (sequence - v_min)/(v_max - v_min)*(new_max - new_min) + new_min\n",
    "    min_max.append([v_min,v_max])\n",
    "    results[index] = v_p\n",
    "  return results, min_max\n",
    "\n",
    "def inverse_min_max(sequences,min_max):\n",
    "  original = []\n",
    "  for index,sequence in enumerate(sequences):\n",
    "    v_min =min_max[index][0]\n",
    "    v_max =min_max[index][1]\n",
    "    sequence_restored = (sequence) * (v_max - v_min) + v_min\n",
    "    original.append(sequence_restored)\n",
    "  return np.array(original)\n",
    "\n",
    "def preprocess_lstm(df):\n",
    "  sequences = list()\n",
    "  for group in df.groupby('Ticker'):\n",
    "    sequences.append(group[1][FEATURE])\n",
    "  sequences=np.array(sequences)\n",
    "\n",
    "  ## min_max trading\n",
    "  price , trade,sentiments =np.split(sequences,[4,5],axis=2)\n",
    "  trade , min_max_trading =min_max(trade)\n",
    "  price , min_max_list =min_max(price)\n",
    "  combine =np.concatenate([price,trade,sentiments],axis =2)\n",
    "\n",
    "  ## min_max another\n",
    "\n",
    "  ## min_max another\n",
    "  result_list= []\n",
    "  for i in range(0,WINDOW_NUMBER):\n",
    "    a, b, c= np.split(combine,[i,i+120],axis=1)\n",
    "    result_list.append(b)\n",
    "  result_array = np.array(result_list)\n",
    "  train ,vaild = np.split(result_array,[46],axis=0)\n",
    "  train_x,train_y=np.split(train.reshape(46*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "  vaild_x,vaild_y=np.split(vaild.reshape(1*TICKER_NUMBER,WINDOW_SIZE+PREDICT_SIZE,FEATURE_SIZE),[WINDOW_SIZE],axis=1)\n",
    "  return train_x,train_y,vaild_x,vaild_y,min_max_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
